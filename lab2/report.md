# 练习1：理解first-fit 连续物理内存分配算法


## 1. 数据结构，函数分析：

### 核心数据结构
- free_area_t：包含
  - free_list：双向循环链表，链入每个空闲块的首页（按地址升序）。
  - nr_free：当前空闲页总数。
- struct Page：每个物理页对应的元信息结构：
  - ref：记录该页框被引用的次数，为0时表示页框可被安全释放。
  - property：若为空闲块首页，记录该块页数；否则为0。
  - flags：通过 PG_property 标志位标识首页。
  - page_link：链表节点，用于把首页加入 free_list。

---

### default_init()
- 作用：初始化 PMM 管理器的内部状态。
- 关键操作：
  - list_init(&free_list)：初始化空闲链表为空。
  - nr_free = 0：清零空闲页计数。
- 何时调用：系统启动早期，用于准备管理器。

---

### default_init_memmap(struct Page *base, size_t n)
- 作用：将从 base 开始、长度为 n 的连续页区加入空闲池。
- 关键操作：
  - 遍历 n 个页，清除 flags、property，set ref = 0（每页初始化）。
  - 在 base 页设置 base->property = n 并设置 PG_property（标记为空闲块首页）。
  - nr_free += n（更新全局空闲页数）。
  - 将 base 的 page_link 按地址顺序插入 free_list（保持链表有序）。
- 备注：进入空闲链表的只有每个空闲块的首页。

---

### default_alloc_pages(size_t n)
- 作用：分配连续的 n 页。
- 搜索过程：
  - 从 free_list 头开始顺序遍历各空闲块首页 p。
  - 找到第一个满足 p->property >= n 的块。
- 分配与分割：
  - 若 p->property == n：将该块从 free_list 中移除（整块分配）。
  - 若 p->property > n：分割，返回 p（首页）作为分配区，剩余部分的首页为 p + n，设置其 property = old - n 并重新插入链表（保持有序位置）。
- 更新：
  - nr_free -= n。
  - 清除分配区首页的 PG_property 标志（该页不再作为空闲块首页）。
- 返回：分配到的首页指针；若无满足块则返回 NULL。

---

### default_free_pages(struct Page *base, size_t n)
- 作用：释放从 base 开始的 n 页并回收到空闲池，尝试合并相邻空闲块。
- 插入前置处理：
  - 遍历 n 页，清 flags，set ref = 0。
  - base->property = n，SetPageProperty(base)，把 base 标记为空闲块首页。
  - nr_free += n。
- 有序插入：
  - 在 free_list 中按地址位置插入 base 的 page_link，保持链表有序。
- 合并：
  - 向前合并：检查插入位置前一个块 p，若 p + p->property == base，则合并（p->property += base->property），移除 base 的链表节点并清除 base 的 PG_property。
  - 向后合并：检查插入位置后一个块 q，若 base + base->property == q，则合并（base->property += q->property），移除 q 的链表节点并清除 q 的 PG_property。
- 结果：通过合并减少碎片、形成更大连续空闲区。

---

## 2.分配/释放总体流程：
- 初始化阶段（pmm_init -> init_pmm_manager -> page_init -> init_memmap）：
  - pmm_init() 调用 init_pmm_manager()，在其中选择具体的 pmm 管理器（默认是 default_pmm_manager）并调用 pmm_manager->init()，对应 default_init()。
  - 随后 page_init() 检测内存边界、计算 npage、设置 pages 数组并标记内核占用页为已保留。
  - page_init() 计算可用物理内存区间并调用 init_memmap(pa2page(mem_begin), cnt)，init_memmap 是对 pmm_manager->init_memmap 的封装，最终由具体实现（如 default_init_memmap）建立 free_list 并更新 nr_free。

- 分配时（调用 alloc_pages -> pmm_manager->alloc_pages -> default_alloc_pages）：
  - 上层通过 alloc_pages(n) 调用 pmm.c 中的包装函数，该函数直接返回 pmm_manager->alloc_pages(n) 的结果。
  - 在 default_alloc_pages(n)中：顺序遍历 free_list，找到第一个 p->property >= n 的空闲块；
    - 若 p->property == n：从 free_list 中删除该块的首页，设置相应标志并返回该页；
    - 若 p->property > n：将该块分割，返回前 n 页作为分配区，剩余部分调整为新的空闲块，更新其 property 和链表位置。
  - 分配完成后更新 nr_free（减去 n）。

- 释放时（调用 free_pages -> pmm_manager->free_pages -> default_free_pages）：
  - 上层通过 free_pages(base, n) 调用 pmm.c 中的包装函数，该函数转发到 pmm_manager->free_pages(base, n)。
  - 在 default_free_pages(base, n) 中：把释放区初始化为一个新的空闲块（base->property = n，SetPageProperty(base)），并按物理地址有序插入 free_list，更新 nr_free（加上 n）。
  - 插入后尝试与前/后相邻空闲块合并，若相邻则合并并更新合并后块的 property，移除被合并节点，以减少碎片。

## 3. First-fit 算法改进空间

### 链表搜索效率
- 问题：每次分配需从头遍历 free_list，最坏时间复杂度为 O(N)。
- 改进：
  - 维护按大小分组的子链表：将空闲块按大小分类或按区间划分，分配时先定位合适大小组，减少扫描范围。

### 外部碎片
- 问题：频繁分配/释放后会产生大量小碎片，导致无法满足大块请求。
- 改进：
  - 阈值分割：只有当剩余空间大于某个最小分割阈值（如 MIN_SPLIT_SIZE）时才对空闲块进行分割，避免产生过小的碎片块。
  - 碎片整理：在碎片总量达到一定阈值时，移动分配给进程的内存分区，以合并那些被阻隔的碎片。



# 练习2：实现 Best-Fit 连续物理内存分配算法


## 1. Best-fit 算法实现过程

### best_fit_init()
- 作用：初始化 PMM 管理器的内部状态。
- 已完成，不需要修改。

---

### best_fit_init_memmap(struct Page *base, size_t n)
- 作用：将从 base 开始、长度为 n 的连续页区加入空闲池。
- 关键操作：
  - 遍历 n 个页，清除 flags、property，set ref = 0（每页初始化）。
  - 将 base 的 page_link 按地址顺序插入 free_list（保持链表有序）。

---

### best_fit_alloc_pages(size_t n)
- 作用：分配连续的 n 页。
- 关键操作：
  - 搜索过程：
    - 新增变量min_size记录当前最佳选择的块大小，初始值为nr_free + 1。
    - 从 free_list 头开始顺序遍历各空闲块首页 p。
    - 找到满足需求(p->property >= n)且比当前最佳选择更合适(p->property < min_size)的块，更新page和min_size。
    -如果找到(p->property = n)的块，提前退出。
  - 分配与分割：
    - 若 p->property > n：分割，返回 p（首页）作为分配区，剩余部分的首页为 p + n，设置其 property  =p->property - n 并重新插入链表（保持有序位置）。


---

### best_fit_free_pages(struct Page *base, size_t n)
- 作用：释放从 base 开始的 n 页并回收到空闲池，尝试合并相邻空闲块。
- 关键操作：
  - 插入前置处理：
    - base->property = n，SetPageProperty(base)，把 base 标记为空闲块首页。
    - nr_free += n。
  - 向前合并：
    - 检查插入位置前一个块 p，若 p + p->property == base，则合并（p->property += base->property），移除 base 的链表节点并清除 base 的 PG_property。


---

### 测试结果：
```
os@yzy13502629353:~/lab2$ make grade
>>>>>>>>>> here_make>>>>>>>>>>>
make[1]: Entering directory '/home/os/lab2' + cc kern/init/entry.S + cc kern/init/init.c + cc kern/libs/stdio.c + cc kern/debug/panic.c + cc kern/driver/dtb.c + cc kern/driver/console.c + cc kern/mm/pmm.c + cc kern/mm/default_pmm.c + cc kern/mm/best_fit_pmm.c + cc libs/printfmt.c + cc libs/sbi.c + cc libs/readline.c + cc libs/string.c + ld bin/kernel riscv64-unknown-elf-objcopy bin/kernel --strip-all -O binary bin/ucore.img make[1]: Leaving directory '/home/os/lab2'
>>>>>>>>>> here_make>>>>>>>>>>>
<<<<<<<<<<<<<<< here_run_qemu <<<<<<<<<<<<<<<<<<
try to run qemu
qemu pid=57357
<<<<<<<<<<<<<<< here_run_check <<<<<<<<<<<<<<<<<<
  -check physical_memory_map_information:    OK
  -check_best_fit:                           OK
Total Score: 25/25
```

## 2. 分配/释放总体流程（Best-fit 实现）：

- 初始化阶段（pmm_init -> init_pmm_manager -> page_init -> init_memmap）：
  - pmm_init() 调用 init_pmm_manager()，在其中选择具体的 pmm 管理器（此处为 best_fit_pmm_manager）并调用 pmm_manager->init()，对应 best_fit_init()，它会初始化空闲链表 free_list 并清零 nr_free。
  - 随后 page_init() 检测物理内存，并为可用内存区域调用 init_memmap()。
  - init_memmap() 最终调用 best_fit_init_memmap()，该函数将新的空闲内存块按物理地址顺序插入到 free_list 中，以保证链表始终有序。

- 分配时（调用 alloc_pages -> pmm_manager->alloc_pages -> best_fit_alloc_pages）：
  - 上层通过 alloc_pages(n) 调用，最终转发至 best_fit_alloc_pages(n)。
  - 搜索：该函数会遍历整个 free_list，寻找一个大小 p->property >= n 且最接近 n 的空闲块（即 p->property 最小）。如果找到大小正好为 n 的块，则提前结束搜索。
  - 分割与分配：
    - 如果找到的最佳块大小 original_block_size 与请求大小 n 的差值大于阈值（代码中为 > n + 5），则进行分割。前 n 页被分配，剩余部分形成一个新的、更小的空闲块，并被重新插入到链表中。nr_free 净减少 n。
    - 如果不满足分割条件，则为了避免产生过小的碎片，将整个块全部分配出去，导致内部碎片。nr_free 减少 original_block_size。
    - 返回分配到的内存块的首页指针。

- 释放时（调用 free_pages -> pmm_manager->free_pages -> best_fit_free_pages）：
  - 上层通过 free_pages(base, n) 调用，最终转发至 best_fit_free_pages(base, n)。
  - 插入：函数首先将要释放的 n 页内存作为一个新的空闲块，按物理地址有序插入到 free_list 中，并更新 nr_free。
  - 迭代合并：
    - 向前合并：循环检查新插入块前面的块是否与其物理地址连续。如果连续，则合并它们（更新前块的 property，删除当前块的节点），并继续向前检查，直到无法合并为止。
    - 向后合并：循环检查合并后块后面的块是否与其物理地址连续。如果连续，则合并它们（更新当前块的 property，删除后块的节点），并继续向后检查，直到无法合并为止。这个过程能有效地将相邻的小碎片合并成大块。


## 3. First-fit 算法改进空间：

### 链表搜索效率
- 问题：每次分配需从头遍历 free_list，最坏时间复杂度为 O(N)。
- 改进：
  - 维护按大小分组的子链表：将空闲块按大小分类或按区间划分，分配时先定位合适大小组，减少扫描范围。

# 扩展练习Challenge：buddy system（伙伴系统）分配算法（需要编程）
## 算法简介
伙伴系统是一种高效的内存管理算法，主要用于减少外部碎片。其核心思想是将物理内存划分为大小均为2的幂次方的块，并通过伙伴合并机制来优化内存使用。

算法核心机制：

- 内存分层：将可用内存按2的幂次方大小划分为多个层级（阶），每个阶维护一个空闲链表

- 分配策略：当请求n页内存时，找到满足2^k ≥ n的最小k阶块。如果该阶无空闲块，则分裂更高阶的块

- 伙伴定义：两个块互为伙伴当且仅当：

  - 大小相同（同一阶）

  - 物理地址连续

  - 起始地址对齐到2^(order+1)边界

- 合并机制：释放内存时，检查伙伴块是否空闲，如果是则合并为更大的块，不断递归直到无法继续合并

- 优缺点：

  - 优点：外部碎片极少，分配/释放效率高（O(logN)）

  - 缺点：可能产生内部碎片，最大连续内存受最大阶限制

## 具体函数
### buddy_system_init
**作用**：初始化伙伴系统的空闲区域数组，为每个阶建立空链表。

**运作流程**：该函数遍历0到MAX_ORDER的所有阶，对每个free_area_buddy[i]调用list_init初始化空闲链表，并将nr_free计数器设为0。这是伙伴系统开始工作前的准备工作。

### buddy_system_init_memmap
**作用**：初始化物理内存映射，将可用内存组织成伙伴系统的数据结构。

**运作流程**：该函数首先初始化所有页面的标志位和引用计数，然后找到能容纳整个内存区域的最大阶，设置基础页面的阶数属性并将其加入对应阶的空闲链表。如果有剩余内存，递归调用自身处理剩余部分。

### buddy_system_alloc_pages
**作用**：分配指定数量的连续物理页面。

**运作流程**：首先计算满足请求的最小阶数，然后从该阶开始向上查找可用的空闲块。找到后从链表中移除，如果块比需要的大，则不断分裂并将伙伴块加入低阶链表，直到得到合适大小的块。

### buddy_system_free_pages
**作用**：释放之前分配的物理页面，尝试合并伙伴块。

**运作流程**：计算被释放块的阶数，设置页面属性，然后循环检查伙伴块是否存在且空闲。如果满足合并条件，则从链表中移除伙伴块，合并成更大的块，直到无法合并或达到最大阶，最后将合并后的块加入对应阶的空闲链表。

### buddy_system_nr_free_pages
**作用**：计算伙伴系统中总的空闲页面数量。

**运作流程**：遍历所有阶的空闲区域，将每个阶的空闲块数量乘以该阶对应的块大小（2^order），累加得到总的空闲页面数。

### buddy_system_print_status
**作用**：打印伙伴系统的当前状态信息，用于调试和监控。

**运作流程**：首先输出总空闲页面数，然后遍历所有阶，对于有空闲块的阶，输出阶数、块大小和空闲块数量，展示内存的分布情况。

### basic_check
**作用**：进行伙伴系统的基本功能验证，确保核心机制正常工作。

**运作流程**：分配和释放单个页面，验证页面引用计数和物理地址的有效性，检查分配失败的情况，验证分配释放后内存守恒，确保基本分配释放操作的正确性。

### buddy_system_check
**作用**：执行全面的伙伴系统功能测试，验证各种边界情况和复杂场景。

**运作流程**：进行8个专项测试，包括基本分配释放、不同大小块分配、伙伴合并验证、内存不足处理、非2的幂分配、碎片整理等。每个测试都包含分配、操作验证和释放步骤，最后检查内存是否守恒，确保所有核心功能正确无误。

### power_of_two
**作用**：计算2的指定次幂，用于确定伙伴系统中各个阶对应的块大小。

**运作流程**：该函数接收一个整数order作为参数，通过左移操作1 << order计算出2的order次方。在伙伴系统中，每个阶对应的大小都是2的幂，如order=0对应1页，order=1对应2页，以此类推。这个计算结果用于确定内存块的大小和进行地址计算。

### log2_ceil
**作用**：计算以2为底的对数并向上取整，用于确定满足内存请求的最小阶数。

**运作流程**：函数通过循环将初始值为1的size不断左移（乘以2），同时order计数器递增，直到size大于或等于输入参数n。返回的order就是满足2^order ≥ n的最小阶数。例如请求3页内存时，返回order=2，因为2^2=4 ≥ 3。

### get_page_order
**作用**：获取页面的阶数属性，表示该页面所属内存块的大小。

**运作流程**：直接返回page结构体的property字段，该字段在伙伴系统中存储了页面所属内存块的阶数。阶数为n表示该块包含2^n个连续页面。

### set_page_order
**作用**：设置页面的阶数属性，标记页面所属内存块的大小。

**运作流程**：将指定的order值赋给page结构体的property字段。这个操作通常在块分裂、合并或初始化时进行，用于维护内存块的元数据信息。

### is_buddy
**作用**：判断两个块是否满足伙伴关系条件，用于合并操作。

**运作流程**：首先检查页面是否具有Property标志，然后比较页面的阶数是否与指定order相等。伙伴关系需要满足三个条件：大小相同、物理地址连续，且起始地址对齐到2^(order+1)边界。

### get_buddy
**作用**：计算给定页面的伙伴块地址，用于合并操作时定位伙伴块。

**运作流程**：通过页面物理地址与块大小（PGSIZE << order）进行异或操作来计算伙伴块的物理地址。如果伙伴地址超出内核内存范围则返回NULL，否则将物理地址转换为页面结构体指针返回。

## 测试用例
### basic_check 测试

**测试内容**：basic_check 是一个基础功能验证测试，主要测试伙伴系统最基本的分配和释放功能。测试内容包括：分配三个单页并验证它们不重叠且地址有效；验证新分配页面的引用计数为0；尝试分配超出可用内存的页面并验证失败；释放所有页面后验证内存恢复到初始状态；验证分配释放操作后伙伴系统的正确性。

**内存预期操作**：在测试过程中，预期伙伴系统会从order=0的链表中分配三个单独的页面，每个分配操作都会从空闲链表中移除对应页面。释放时，这些页面应该被重新插入回order=0的空闲链表。测试还验证了当分配大块内存失败时，系统能正确处理而不会破坏内存结构。最后测试确保分配释放循环后，系统的空闲内存总量保持不变。

### buddy_system_check 测试
下面是全面的测试，包含8个小测试：基本分配释放、不同大小块分配、伙伴合并验证、内存不足处理、精确大小分配、详细分配测试、非2的幂分配、碎片整理测试。测试覆盖了各种边界情况，包括不同大小的内存块分配、伙伴关系的验证、内存碎片化场景的处理等。整个测试过程中，系统应保持内存守恒，即测试开始和结束时的总空闲页面数相同。

### 测试1：基本分配释放

**测试内容**：测试单个页面的分配和释放基本功能，验证最基本的分配释放操作是否正常工作。测试分配一个页面并确认成功，然后释放该页面，检查系统状态是否恢复正常。

**内存预期操作**：预期系统从order=0的空闲链表中分配一个页面，分配后该链表节点减少。释放时页面应正确返回到order=0的空闲链表中，空闲页面计数恢复原状。

### 测试2：分配不同大小的块

**测试内容**：测试同时分配不同大小的内存块（2页、4页、8页），验证系统能正确处理多种大小的并发分配请求。测试分配成功后立即释放所有块。

**内存预期操作**：预期系统分别从order=1、order=2、order=3的空闲链表中分配对应大小的块。如果某些阶的链表为空，系统应从更高阶分裂块来满足需求。释放时各块应回到对应的空闲链表中。

### 测试3：伙伴合并测试

**测试内容**：专门测试伙伴系统的伙伴合并功能。通过分配一个2页块，然后检查其内部两个页面是否构成伙伴关系，验证释放后是否能正确合并。

**内存预期操作**：分配2页块时，系统可能从order=1直接分配或从更高阶分裂。释放时，两个相邻的1页块（伙伴块）应该检测到彼此都空闲，从而合并成一个2页块并提升到order=1的空闲链表中。

### 测试4：内存不足测试

**测试内容**：测试系统在内存不足情况下的行为，尝试分配超过当前可用内存总量的超大块，验证系统是否能正确拒绝此类请求而不会崩溃或返回无效指针。

**内存预期操作**：预期系统遍历所有阶的空闲链表后找不到足够大的连续内存块，应返回NULL指针而不修改任何内存结构。所有空闲链表和页面属性应保持不变。

### 测试5：精确大小分配

**测试内容**：测试分配正好是2的幂大小的内存块（16页），验证系统对精确大小请求的处理效率，检查是否避免了不必要的块分裂。

**内存预期操作**：预期系统直接从order=4的空闲链表中分配16页的完整块，不需要进行任何分裂操作。释放时该块也应完整返回到order=4链表中，不需要合并操作。

### 测试6：详细分配测试

**测试内容**：进行混合大小的分配测试（1页、2页、4页），分配后打印系统状态，然后释放所有块，验证复杂分配模式下的系统行为。

**内存预期操作**：预期系统正确处理不同大小的并发分配，可能涉及多个阶的块分裂。状态打印应显示相应阶的空闲块减少。释放后所有块应正确回到对应阶的空闲链表中。

### 测试7：非2的幂分配

**测试内容**：测试请求非2的幂大小的内存分配（3页、5页、7页），验证系统的向上取整机制，检查实际分配的内存块大小是否符合预期。

**内存预期操作**：预期系统将3页请求向上取整到4页（order=2），5页和7页请求向上取整到8页（order=3）。分配时可能需要进行块分裂，释放时系统应正确处理非精确大小的释放请求。

### 测试8：碎片整理测试

**测试内容**：模拟内存碎片化场景，交替分配不同大小的块（1页、2页、4页循环），然后交错释放，最后尝试分配大块内存，测试系统的碎片整理能力。

**内存预期操作**：预期在交错释放后产生内存碎片，但当尝试分配8页大块时，系统应能通过伙伴合并机制将相邻的小空闲块合并成足够大的块。这验证了伙伴系统在长期运行后仍能通过合并减少外部碎片。

# 扩展练习Challenge：buddy system（伙伴系统）分配算法（需要编程）
## 算法简介

SLUB是一种高效的小内存分配器，作为SLAB分配器的改进版本，它简化了设计并提高了性能。SLUB的核心思想是为不同大小的对象创建专用缓存，每个缓存维护三个页面链表：full（完全分配）、partial（部分分配）和empty（完全空闲）。分配时优先从partial链表获取，释放时根据页面状态动态移动链表位置。SLUB基于伙伴系统获取大块内存，然后将其分割成相同大小的小对象，通过空闲对象链表管理可用内存。这种设计特别适合内核中小对象的频繁分配和释放场景，减少了内存碎片并提高了分配效率。

## 具体函数

### slub_align_size
**作用**：计算对齐后的对象大小，确保内存地址按照指针大小对齐，提高内存访问效率。

**运作流程**：该函数接收原始大小参数，通过`(size + sizeof(void *) - 1) & ~(sizeof(void *) - 1)`计算方式实现向上对齐。例如在32位系统中，sizeof(void *)为4字节，该计算确保返回的大小是4的倍数，这样每个对象在内存中都有正确的对齐，避免了未对齐内存访问的性能损失。

### slub_find_cache_index
**作用**：根据请求的内存大小找到最适合的SLUB缓存索引，实现大小分类管理。

**运作流程**：函数遍历所有已初始化的SLUB缓存，比较每个缓存的对象大小与请求大小，返回第一个对象大小大于等于请求大小的缓存索引。如果找不到合适的缓存则返回-1，这种查找机制确保了每个内存请求都能被分配到最合适大小的缓存中。

### slub_alloc_page_for_cache
**作用**：从伙伴系统分配物理页面并将其初始化为SLUB缓存页，构建空闲对象链表。

**运作流程**：首先调用伙伴系统分配2^order个页面，然后在页面开头存储slub_page元数据，剩余空间分割成多个相同大小的对象。通过链表连接所有空闲对象，freelist指向第一个空闲对象，每个空闲对象的开头存储下一个空闲对象的指针，形成单链表结构。

### slub_free_page_for_cache
**作用**：将SLUB缓存页面释放回伙伴系统，回收大块内存。

**运作流程**：直接调用伙伴系统的free_pages函数，传入页面指针和阶数参数。由于SLUB页面来自伙伴系统，释放时只需要归还给原系统即可，伙伴系统会负责后续的合并等管理操作。

### slub_add_page_to_list
**作用**：将SLUB页面添加到指定链表的头部，维护页面链表结构。

**运作流程**：函数接收链表头指针和页面指针参数，将页面的slub_page结构中的next指针指向当前链表头，然后更新链表头指针指向新添加的页面。这种头插法操作时间复杂度为O(1)，保证了链表操作的高效性。

### slub_remove_page_from_list
**作用**：从指定链表中移除目标页面，保持链表结构的完整性。

**运作流程**：遍历链表查找目标页面，找到后调整前后节点的指针关系，将目标页面从链表中移除。遍历过程中维护prev指针记录前驱节点，当找到目标节点时，让前驱节点的next指向目标节点的next，完成移除操作。

### slub_get_page_state
**作用**：判断SLUB页面的当前状态（满、部分满、空），用于页面链表管理。

**运作流程**：检查页面的inuse计数器，如果inuse为0表示页面为空闲状态（SLUB_EMPTY），如果inuse等于总对象数表示页面已满（SLUB_FULL），其他情况为部分使用状态（SLUB_PARTIAL）。这种状态判断是页面在三个链表间移动的依据。

### slub_move_page
**作用**：根据页面状态将其移动到正确的链表，维护SLUB的三链表结构。

**运作流程**：首先从所有可能链表中移除该页面，然后根据当前状态调用slub_add_page_to_list将其添加到对应链表。这种先移除后添加的方式确保了页面始终处于正确的链表中，反映了页面的真实使用状态。

### slub_find_page_in_list
**作用**：在指定链表中查找包含给定地址的页面，用于内存释放时定位所属页面。

**运作流程**：遍历链表中的每个slub_page，计算每个页面的起始和结束地址范围，检查目标地址是否在该范围内。找到包含该地址的页面后，通过pa2page转换返回对应的Page结构，用于后续的释放操作。

### slub_alloc_pages_small
**作用**：处理小内存分配请求，从合适的SLUB缓存中分配对象。

**运作流程**：首先检查请求大小是否超过SLUB处理范围，超过则直接使用伙伴系统。否则找到合适缓存，优先从partial链表分配，其次empty链表，都需要时申请新页面。从页面的freelist获取空闲对象，更新使用计数，移动页面到正确链表，最后清零内存并返回对象指针。

### slub_free_pages_small
**作用**：释放小内存对象，将其返回到对应的SLUB缓存中。

**运作流程**：首先检查是否为大内存直接释放，否则遍历所有缓存查找包含该对象的页面。找到后将被释放对象插回页面的freelist头部，减少inuse计数，移动页面到合适链表。如果页面完全空闲且空页面过多，则释放该页面回伙伴系统以节省内存。

### slub_init
**作用**：初始化SLUB分配器，创建不同大小的缓存池。

**运作流程**：预定义8种对象大小（32B到4096B），为每个大小创建slub_cache结构。计算合适的order值确保每页至少有4个对象，初始化三个链表头为NULL，设置统计信息归零，最后打印初始化信息。

### slub_init_memmap
**作用**：初始化内存映射，将具体工作委托给伙伴系统。

**运作流程**：直接调用buddy_system_init_memmap函数，因为SLUB建立在伙伴系统之上，不需要单独处理物理内存的初始映射。

### slub_alloc_pages
**作用**：统一的内存分配接口，根据请求大小选择SLUB或伙伴系统。

**运作流程**：判断请求的页面数是否超过SLUB处理阈值，超过则直接调用伙伴系统分配。否则调用slub_alloc_pages_small进行小内存分配，然后将返回的对象地址转换为对应的Page结构返回。

### slub_free_pages
**作用**：统一的内存释放接口，根据内存类型选择相应的释放策略。

**运作流程**：检查页面属性或请求大小，如果满足大内存条件则直接调用伙伴系统释放。否则通过slub_free_pages_small处理小内存释放，这种区分处理优化了不同大小内存的管理效率。

### slub_nr_free_pages
**作用**：获取系统总的空闲页面数量，反映内存使用情况。

**运作流程**：直接返回伙伴系统的nr_free_pages结果，因为SLUB使用的内存都来自伙伴系统，这种设计避免了重复统计，保证了内存计数的一致性。

### slub_print_status
**作用**：打印SLUB分配器的详细状态信息，用于调试和监控。

**运作流程**：首先显示伙伴系统的总空闲页面，然后遍历所有缓存，统计每个缓存的full、partial、empty链表中的页面数量，最后显示每个缓存的活跃对象数，展示SLUB的内存分布和使用情况。

## 测试用例
## slub_check 测试用例

以下是一个全面的SLUB分配器集成测试，包含5个专项测试：基本分配释放、不同大小分配、小内存分配测试、大量对象分配、混合分配测试。测试覆盖了SLUB的各种使用场景，包括页面级分配、小对象分配、数据完整性验证、批量操作和混合内存管理策略。预期SLUB能正确处理各种分配模式，并且所有测试完成后应保持内存守恒，无内存泄漏。

## 测试1：基本分配释放

**测试内容**：测试SLUB最基本的页面分配和释放功能，验证单个页面的分配是否成功，释放后系统状态是否恢复正常。这是对SLUB与伙伴系统协作的基本验证。

**内存预期操作**：预期分配时SLUB调用伙伴系统获取页面，释放时将页面归还给伙伴系统。整个过程应保持内存计数正确，分配返回有效页面指针，释放后空闲页面数恢复初始值。

## 测试2：不同大小分配

**测试内容**：测试连续分配多个相同大小的页面，并在分配后向页面写入测试数据验证内存可写性，然后释放所有页面。主要验证SLUB处理连续分配和内存可用性的能力。

**内存预期操作**：预期SLUB能成功分配多个页面，每个页面都来自伙伴系统。写入测试数据时应无异常，证明分配的内存可正常使用。释放时所有页面应正确归还，内存状态恢复。

## 测试3：小内存分配测试

**测试内容**：专门测试SLUB的小对象分配功能，分配64B、256B、1024B等不同大小的对象，验证SLUB缓存机制的正确性。包括分配成功性验证和内存写入测试。

**内存预期操作**：预期不同大小的对象从对应的SLUB缓存中分配：64B从size-64缓存、256B从size-256缓存、1024B从size-1024缓存分配。分配时应从对应缓存的partial或empty链表获取对象，释放时对象应回到页面的freelist中。

## 测试4：大量对象分配

**测试内容**：进行压力测试，批量分配20个128B大小的对象，验证数据完整性，进行交错释放和重新分配操作。测试SLUB处理大量小对象的能力和内存管理稳定性。

**内存预期操作**：预期SLUB从size-128缓存分配多个页面来容纳20个对象，可能涉及新页面的申请。交错释放时部分页面应从full状态变为partial状态，重新分配时应能重用释放的对象。数据完整性验证确保内存访问正确。

## 测试5：混合分配测试

**测试内容**：测试SLUB同时处理页面级分配和小对象分配的混合场景，验证两种分配策略的协同工作能力。包括分配2个页面和512B小对象，然后正确释放。

**内存预期操作**：预期页面分配直接通过伙伴系统完成，而小对象从size-512缓存分配。释放时页面直接归还伙伴系统，小对象回到SLUB缓存。这种混合操作验证了SLUB能正确区分处理不同大小的内存请求。